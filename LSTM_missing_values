import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

data = "fed-funds-rate-historical-chart.csv"
df = pd.read_csv(data, skiprows=16, names=['date', 'value'])

df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.dropna(subset=['value'])

all_dates = pd.date_range(start="1954-07-01", end="2024-11-01")
complete_df = pd.DataFrame({'date': all_dates})

merged_df = pd.merge(complete_df, df, on='date', how='left')

# Normalize the data (important for LSTM)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_values = scaler.fit_transform(df[['value']])

def create_dataset(data, time_step=1):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step), 0])
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

time_step = 30  
X, y = create_dataset(scaled_values, time_step)


X = X.reshape(X.shape[0], X.shape[1], 1)


train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(time_step, 1)))
model.add(LSTM(units=50))
model.add(Dense(units=1))

model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model with early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])

# Predict the federal fund rates
#predicted_values = model.predict(X_test)

window_data = np.array(df['value'].iloc[-time_step:]) 

window_data_scaled = scaler.transform(window_data.reshape(-1, 1))  
window_data_scaled = window_data_scaled.reshape(1, time_step, 1) 

predicted_value = model.predict(window_data_scaled)

predicted_value = scaler.inverse_transform(predicted_value)

print(f"Predicted value: {predicted_value[0][0]}")

#predicted_values = scaler.inverse_transform(predicted_values)

missing_dates_df = merged_df[merged_df['value'].isnull()]

missing_dates = missing_dates_df['date'].values
missing_indices = merged_df.index[merged_df['value'].isnull()].tolist()

for idx in missing_indices:
    window_data = merged_df.iloc[idx - time_step: idx]['value'].values
    window_data_scaled = scaler.transform(window_data.reshape(-1, 1))
    window_data_scaled = window_data_scaled.reshape(1, time_step, 1)  # reshape for LSTM input
    prediction_scaled = model.predict(window_data_scaled)
    predicted_value = scaler.inverse_transform(prediction_scaled)

    merged_df.at[idx, 'value'] = predicted_value[0][0]


# Filter the rows around 2017-12-25
dates_around_2017_12_25 = merged_df[(merged_df['date'] >= '2017-12-20') & (merged_df['date'] <= '2017-12-30')]

# Display the prediction for 12/25/2017
print("Dates and values around 2017-12-25:")
print(dates_around_2017_12_25)
